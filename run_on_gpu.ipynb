{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用cpu训练，自己计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at begin:29700834.0\n",
      "loss after 499 epoch:5.46413502888754e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device（“cuda：0”）＃取消注释以在GPU上运行\n",
    "\n",
    "# N是批量大小; D_in是输入维度;\n",
    "# H是隐藏的维度; D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "#创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传递：计算预测y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 计算和打印损失\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    \n",
    "    if t == 0:\n",
    "        print(f'loss at begin:{loss}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss}')\n",
    "\n",
    "    # Backprop计算w1和w2相对于损耗的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.改成自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at begin:21229844.0\n",
      "loss after 499 epoch:7.07513972884044e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device（“cuda：0”）＃取消注释以在GPU上运行\n",
    "\n",
    "# N是批量大小; D_in是输入维度;\n",
    "# H是隐藏的维度; D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机Tensors以保持输入和输出。\n",
    "# 设置requires_grad = False表示我们不需要计算渐变\n",
    "# 在向后传球期间对于这些Tensors。\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 为权重创建随机Tensors。\n",
    "# 设置requires_grad = True表示我们想要计算渐变\n",
    "# 在向后传球期间尊重这些张贴。\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：使用tensors上的操作计算预测值y; \n",
    "    # 由于w1和w2有requires_grad=True，涉及这些张量的操作将让PyTorch构建计算图，\n",
    "    # 从而允许自动计算梯度。由于我们不再手工实现反向传播，所以不需要保留中间值的引用。\n",
    "    # clamp就是将数据规范到某个范围内，min=0即小于0的值就直接取0，这就模拟了relu的效果\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # 使用Tensors上的操作计算和打印丢失。\n",
    "    # loss是一个形状为()的张量\n",
    "    # loss.item() 得到这个张量对应的python数值\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t == 0:\n",
    "        print(f'loss at begin:{loss}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss}')\n",
    "\n",
    "    # 使用autograd计算反向传播。这个调用将计算loss对所有requires_grad=True的tensor的梯度。\n",
    "    # 这次调用后，w1.grad和w2.grad将分别是loss对w1和w2的梯度张量。\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用梯度下降更新权重。对于这一步，我们只想对w1和w2的值进行原地改变；不想为更新阶段构建计算图，\n",
    "    # 所以我们使用torch.no_grad()上下文管理器防止PyTorch为更新构建计算图\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 反向传播后手动将梯度设置为零\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义求导函数\n",
    "# 我们可以很容易地通过定义torch.autograd.Function的子类并实现forward和backward函数，来定义自己的自动求导运算。\n",
    "# 之后我们就可以使用这个新的自动梯度运算符了。然后，我们可以通过构造一个实例并像调用函数一样，传入包含输入数据的tensor调用它，这样来使用新的自动求导运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at begin:31408072.0\n",
      "loss after 499 epoch:7.993909093784168e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数，\n",
    "    并完成张量的正向和反向传播。\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        在正向传播中，我们接收到一个上下文对象和一个包含输入的张量；\n",
    "        我们必须返回一个包含输出的张量，\n",
    "        并且我们可以使用上下文对象来缓存对象，以便在反向传播中使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收到上下文对象和一个张量，\n",
    "        其包含了相对于正向传播过程中产生的输出的损失的梯度。\n",
    "        我们可以从上下文对象中检索缓存的数据，\n",
    "        并且必须计算并返回与正向传播的输入相关的损失的梯度。\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# N是批大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# 产生随机权重的张量\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 正向传播：使用张量上的操作来计算输出值y；\n",
    "    # 我们通过调用 MyReLU.apply 函数来使用自定义的ReLU\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t == 0:\n",
    "        print(f'loss at begin:{loss.item()}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss.item()}')\n",
    "\n",
    "    # 使用autograd计算反向传播过程。\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 用梯度下降更新权重\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 在反向传播之后手动清零梯度\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS：也相当于定义了一个自定义层的感觉？ 还要自己写反向传播，感觉很麻烦的样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在tensorflow中做，体会eager和静态的区别\n",
    "# ps: 这里的tensorflow version < 2.0, 2.0以上tensorflow也默认开启eager模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7c411412def8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 为输入和目标数据创建placeholder；\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 当执行计算图时，他们将会被真实的数据填充\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 首先我们建立计算图（computational graph）\n",
    "\n",
    "# N是批大小；D是输入维度；\n",
    "# H是隐藏层维度；D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 为输入和目标数据创建placeholder；\n",
    "# 当执行计算图时，他们将会被真实的数据填充\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# 为权重创建Variable并用随机数据初始化\n",
    "# TensorFlow的Variable在执行计算图时不会改变\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# 前向传播：使用TensorFlow的张量运算计算预测值y。\n",
    "# 注意这段代码实际上不执行任何数值运算；\n",
    "# 它只是建立了我们稍后将执行的计算图。\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# 使用TensorFlow的张量运算损失（loss）\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# 计算loss对于w1和w2的导数\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# 使用梯度下降更新权重。为了实际更新权重，我们需要在执行计算图时计算new_w1和new_w2。\n",
    "# 注意，在TensorFlow中，更新权重值的行为是计算图的一部分;\n",
    "# 但在PyTorch中，这发生在计算图形之外。\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# 现在我们搭建好了计算图，所以我们开始一个TensorFlow的会话（session）来实际执行计算图。\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # 运行一次计算图来初始化Variable w1和w2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 创建numpy数组来存储输入x和目标y的实际数据\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "\n",
    "    for _ in range(500):\n",
    "        # 多次运行计算图。每次执行时，我们都用feed_dict参数，\n",
    "        # 将x_value绑定到x，将y_value绑定到y，\n",
    "        # 每次执行图形时我们都要计算损失、new_w1和new_w2；\n",
    "        # 这些张量的值以numpy数组的形式返回。\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2], \n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        if t == 0:\n",
    "        print(f'loss at begin:{loss_value}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 这么搞显然显得麻烦，如同TensorFlow中有keras和tensorflow-slim等封装得非常好的抽象接口一样\n",
    "# pytorch中 nn 模块也是来完成同样功能。\n",
    "# nn包中定义一组大致等价于层的模块。一个模块接受输入的tesnor，计算输出的tensor，而且 还保存了一些内部状态比如需要学习的tensor的参数等。\n",
    "# nn包中也定义了一组损失函数（loss functions），用来训练神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at begin:726.6447143554688\n",
      "loss after 499 epoch:1.844366011027887e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N是批大小；D是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "#创建输入和输出随机张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用nn包将我们的模型定义为一系列的层。\n",
    "# nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出。\n",
    "# 每个线性模块使用线性函数从输入计算输出，并保存其内部的权重和偏差张量。\n",
    "# 在构造模型之后，我们使用.to()方法将其移动到所需的设备。\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# nn包还包含常用的损失函数的定义；\n",
    "# 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数。\n",
    "# 设置reduction='sum'，表示我们计算的是平方误差的“和”，而不是平均值;\n",
    "# 这是为了与前面我们手工计算损失的例子保持一致，\n",
    "# 但是在实践中，通过设置reduction='elementwise_mean'来使用均方误差作为损失更为常见。\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 前向传播：通过向模型传入x计算预测的y。\n",
    "    # 模块对象重载了__call__运算符，所以可以像函数那样调用它们。\n",
    "    # 这么做相当于向模块传入了一个张量，然后它返回了一个输出张量。\n",
    "    y_pred = model(x)\n",
    "\n",
    "     # 计算并打印损失。\n",
    "     # 传递包含y的预测值和真实值的张量，损失函数返回包含损失的张量。\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t == 0:\n",
    "        print(f'loss at begin:{loss.item()}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss.item()}')\n",
    "\n",
    "\n",
    "    # 反向传播之前清零梯度\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 反向传播：计算模型的损失对所有可学习参数的导数（梯度）。\n",
    "    # 在内部，每个模块的参数存储在requires_grad=True的张量中，\n",
    "    # 因此这个调用将计算模型中所有可学习参数的梯度。\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用梯度下降更新权重。\n",
    "    # 每个参数都是张量，所以我们可以像我们以前那样可以得到它的数值和梯度\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如上，网络结构的定义变得灰常简单，但更新方式能不能也简化一点呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当然是阔以的，毕竟对于随机梯度下降(SGD/stochastic gradient descent)等简单的优化算法来说，这不是一个很大的负担\n",
    "# 但在实践中，我们经常使用AdaGrad、RMSProp、Adam等更复杂的优化器来训练神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at begin:703.8837890625\n",
      "loss after 499 epoch:6.641418526243115e-10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N是批大小；D是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用nn包定义模型和损失函数\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 使用optim包定义优化器（Optimizer）。Optimizer将会为我们更新模型的权重。\n",
    "# 这里我们使用Adam优化方法；optim包还包含了许多别的优化算法。\n",
    "# Adam构造函数的第一个参数告诉优化器应该更新哪些张量。\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "\n",
    "    # 前向传播：通过像模型输入x计算预测的y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t == 0:\n",
    "        print(f'loss at begin:{loss.item()}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss.item()}')\n",
    "\n",
    "    # 在反向传播之前，使用optimizer将它要更新的所有张量的梯度清零(这些张量是模型可学习的权重)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 反向传播：根据模型的参数计算loss的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 调用Optimizer的step函数使它所有参数更新\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义层，通过集成nn.Module并实现forward函数来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at begin:695.1787109375\n",
      "loss after 499 epoch:0.00018011558859143406\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。\n",
    "        我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# N是批大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 通过实例化上面定义的类来构建我们的模型。\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 构造损失函数和优化器。\n",
    "# SGD构造函数中对model.parameters()的调用，\n",
    "# 将包含模型的一部分，即两个nn.Linear模块的可学习参数。\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 前向传播：通过向模型传递x计算预测值y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    #计算并输出loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t == 0:\n",
    "        print(f'loss at begin:{loss.item()}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss.item()}')\n",
    "\n",
    "    # 清零梯度，反向传播，更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态图和权重共享的展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at begin:561.6505737304688\n",
      "loss after 499 epoch:0.5097711086273193\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们构造了三个nn.Linear实例，它们将在前向传播时被使用。\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        对于模型的前向传播，我们随机选择0、1、2、3，\n",
    "        并重用了多次计算隐藏层的middle_linear模块。\n",
    "        由于每个前向传播构建一个动态计算图，\n",
    "        我们可以在定义模型的前向传播时使用常规Python控制流运算符，如循环或条件语句。\n",
    "        在这里，我们还看到，在定义计算图形时多次重用同一个模块是完全安全的。\n",
    "        这是Lua Torch的一大改进，因为Lua Torch中每个模块只能使用一次。\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N是批大小；D是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出随机张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 实例化上面定义的类来构造我们的模型\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 构造我们的损失函数（loss function）和优化器（Optimizer）。\n",
    "# 用平凡的随机梯度下降训练这个奇怪的模型是困难的，所以我们使用了momentum方法。\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "\n",
    "    # 前向传播：通过向模型传入x计算预测的y。\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印损失\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    if t == 0:\n",
    "        print(f'loss at begin:{loss.item()}')\n",
    "    if t == 499:\n",
    "        print(f'loss after {t} epoch:{loss.item()}')\n",
    "\n",
    "    # 清零梯度，反向传播，更新权重 \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
